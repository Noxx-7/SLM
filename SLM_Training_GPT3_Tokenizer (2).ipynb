{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Advanced SLM Training with GPT-3 Tokenizer\n",
    "\n",
    "## \u2705 Complete Training System:\n",
    "- **Multi-GPU Support**: DataParallel for Kaggle T4\u00d72 GPUs\n",
    "- **GPT-3 Tokenizer**: Using tiktoken (cl100k_base) - same as GPT-3.5/GPT-4\n",
    "- **Multi-Dataset Training**: WikiText-2, Sherlock Holmes (Project Gutenberg), Numerical samples\n",
    "- **Enhanced Config for T4\u00d72**: 768 emb, 8 layers, 8 heads, 3072 FFN, 1024 context, FP16, gradient checkpointing\n",
    "- **Enhanced .pt Files**: Complete architecture + metadata + tokenizer + training history\n",
    "- **Automatic Fine-Tuning**: Literary datasets after base training\n",
    "- **Cross-Platform**: Auto-detects Kaggle/Colab/local environments\n",
    "\n",
    "## \ud83d\udce6 What's in the final.pt File:\n",
    "- \u2705 Complete model architecture (RMSNorm, RoPE, SwiGLU, MultiHeadAttention)\n",
    "- \u2705 All model weights and state dict\n",
    "- \u2705 Complete hyperparameters and training configuration\n",
    "- \u2705 GPT-3 tokenizer configuration and vocabulary info\n",
    "- \u2705 Training metadata (timestamps, epochs, loss history)\n",
    "- \u2705 Version tracking and dataset information\n",
    "- \u2705 Ready for future retraining with additional datasets\n",
    "\n",
    "## \ud83d\udd04 Training Workflow:\n",
    "1. **Stage 1**: Train base model on WikiText-2 + Sherlock Holmes + Numerical data\n",
    "2. **Stage 2**: Save comprehensive final.pt with full metadata\n",
    "3. **Stage 3**: Automatically load and fine-tune on additional literary data\n",
    "4. **Stage 4**: Save final enhanced model ready for deployment\n",
    "\n",
    "**Run all cells sequentially to complete the full training pipeline!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Step 1: Install Packages & Detect Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch>=2.0.0 transformers>=4.30.0 datasets>=2.14.0 tqdm>=4.65.0 huggingface_hub>=0.16.0 accelerate>=0.20.0 psutil tiktoken>=0.5.0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import uuid\n",
    "import shutil\n",
    "import psutil\n",
    "import gc\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Any\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"\u2705 Packages installed!\")\n",
    "\n",
    "def detect_environment():\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    elif 'COLAB_GPU' in os.environ or 'google.colab' in str(sys.modules):\n",
    "        return 'colab'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "ENV = detect_environment()\n",
    "print(f\"\\n\ud83c\udf0d Platform detected: {ENV.upper()}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\n\ud83d\udda5\ufe0f  Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Step 1.5: Multi-GPU Setup with DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU Setup - Using DataParallel for Jupyter notebooks\n",
    "USE_MULTI_GPU = torch.cuda.device_count() > 1\n",
    "USE_DDP = False  # DDP not suitable for notebooks, using DataParallel instead\n",
    "\n",
    "if USE_MULTI_GPU:\n",
    "    print(f\"\ud83d\ude80 Multiple GPUs detected: {torch.cuda.device_count()} GPUs\")\n",
    "    print(\"   Using DataParallel for multi-GPU training in notebook environment\")\n",
    "    # DataParallel will be applied to model after creation\n",
    "else:\n",
    "    print(f\"\ud83d\udcbb Single GPU mode: Using {device}\")\n",
    "\n",
    "# Adjust batch size for multi-GPU\n",
    "if USE_MULTI_GPU:\n",
    "    # Use 4 per GPU for larger model\n",
    "    effective_batch_size = 4 * torch.cuda.device_count()\n",
    "    print(f\"   Effective batch size: {effective_batch_size} (4 per GPU)\")\n",
    "else:\n",
    "    effective_batch_size = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\uddc2\ufe0f Step 2: Configure Paths & Resource Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd27 Configuring paths for\", ENV.upper())\n",
    "\n",
    "if ENV == 'kaggle':\n",
    "    BASE_DIR = \"/kaggle/working\"\n",
    "elif ENV == 'colab':\n",
    "    BASE_DIR = \"/content\"\n",
    "else:\n",
    "    BASE_DIR = \".\"\n",
    "\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "\n",
    "for dir_path in [CACHE_DIR, MODEL_DIR, DATA_DIR, CHECKPOINT_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Directories created:\")\n",
    "print(f\"   Models: {MODEL_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Cache: {CACHE_DIR}\")\n",
    "\n",
    "def get_disk_usage(path=None):\n",
    "    if path is None:\n",
    "        path = BASE_DIR\n",
    "    if os.path.exists(path):\n",
    "        total, used, free = shutil.disk_usage(path)\n",
    "    else:\n",
    "        total, used, free = shutil.disk_usage(\".\")\n",
    "    return {\"total_gb\": total / (1024**3), \"used_gb\": used / (1024**3), \n",
    "            \"free_gb\": free / (1024**3), \"used_percent\": (used / total) * 100}\n",
    "\n",
    "def monitor_resources():\n",
    "    disk = get_disk_usage()\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"\\n\ud83d\udcca Resources:\")\n",
    "    print(f\"   Disk: {disk['free_gb']:.1f}GB free / {disk['total_gb']:.1f}GB total\")\n",
    "    print(f\"   RAM: {memory.percent:.0f}% used\")\n",
    "    if device.startswith('cuda'):\n",
    "        for gpu_id in range(torch.cuda.device_count()):\n",
    "            gpu_mem = torch.cuda.memory_allocated(gpu_id) / (1024**3)\n",
    "            gpu_total = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**3)\n",
    "            print(f\"   GPU {gpu_id}: {gpu_mem:.1f}GB / {gpu_total:.1f}GB used\")\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "monitor_resources()\n",
    "print(\"\\n\u2705 Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd24 Step 3: Initialize GPT-3 Tokenizer (tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd24 Initializing GPT-3 tokenizer (tiktoken)...\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "\n",
    "print(f\"\u2705 GPT-3 Tokenizer loaded!\")\n",
    "print(f\"   Encoding: cl100k_base (same as GPT-3.5/GPT-4)\")\n",
    "print(f\"   Vocab size: {VOCAB_SIZE:,}\")\n",
    "\n",
    "test_text = \"Hello, World! 12345\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"\\n   Test encoding: '{test_text}'\")\n",
    "print(f\"   Tokens: {tokens}\")\n",
    "print(f\"   Decoded: '{decoded}'\")\n",
    "print(f\"   Token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Step 4: Complete Model Architecture (Llama3-Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / norm * self.weight\n",
    "\n",
    "def precompute_freqs_cis(dim: int, max_len: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(max_len)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cos, freqs_sin):\n",
    "    xq_r = xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
    "    xk_r = xk.float().reshape(*xk.shape[:-1], -1, 2)\n",
    "    xq_out_r = xq_r[..., 0] * freqs_cos - xq_r[..., 1] * freqs_sin\n",
    "    xq_out_i = xq_r[..., 0] * freqs_sin + xq_r[..., 1] * freqs_cos\n",
    "    xk_out_r = xk_r[..., 0] * freqs_cos - xk_r[..., 1] * freqs_sin\n",
    "    xk_out_i = xk_r[..., 0] * freqs_sin + xk_r[..., 1] * freqs_cos\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(-2)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(-2)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = int(2 * dim * 4 / 3)\n",
    "            hidden_dim = ((hidden_dim + 63) // 64) * 64\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert emb_size % n_heads == 0\n",
    "        self.emb_size = emb_size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_size // n_heads\n",
    "        self.qkv = nn.Linear(emb_size, 3 * emb_size, bias=False)\n",
    "        self.out = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.freqs_cos, self.freqs_sin = precompute_freqs_cis(self.head_dim, max_len)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        self.freqs_cos = self.freqs_cos.to(x.device)\n",
    "        self.freqs_sin = self.freqs_sin.to(x.device)\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cos[:L], self.freqs_sin[:L])\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
    "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().reshape(B, L, self.emb_size)\n",
    "        return self.out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads, ff_size, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(emb_size, n_heads, max_len, dropout)\n",
    "        self.ff = SwiGLU(emb_size, ff_size)\n",
    "        self.norm1 = RMSNorm(emb_size)\n",
    "        self.norm2 = RMSNorm(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x), mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class AdvancedLLM(nn.Module):\n",
    "    def __init__(self, vocab_size=100277, emb_size=768, n_layers=8, n_heads=8, \n",
    "                 ff_size=3072, max_len=1024, dropout=0.1, use_gradient_checkpoint=True):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.max_len = max_len\n",
    "        self.use_gradient_checkpoint = use_gradient_checkpoint\n",
    "        self.token_embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(emb_size, n_heads, ff_size, max_len, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(emb_size)\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size, bias=False)\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L = x.shape\n",
    "        if mask is None:\n",
    "            mask = torch.tril(torch.ones(L, L, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        x = self.dropout(self.token_embedding(x))\n",
    "        for block in self.blocks:\n",
    "            if self.use_gradient_checkpoint and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(block, x, mask, use_reentrant=False)\n",
    "            else:\n",
    "                x = block(x, mask)\n",
    "        return self.lm_head(self.norm(x))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "print(\"\u2705 Model architecture loaded: RMSNorm, RoPE, SwiGLU, Gradient Checkpointing\")\n",
    "print(f\"   Configuration: 768 emb, 8 layers, 8 heads, 3072 ff_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Step 5: Load and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcda Loading datasets...\")\n",
    "\n",
    "def download_sherlock_holmes():\n",
    "    print(\"\\n\ud83d\udcd6 Downloading Sherlock Holmes from Project Gutenberg...\")\n",
    "    urls = [\n",
    "        \"https://www.gutenberg.org/files/1661/1661-0.txt\",\n",
    "        \"https://www.gutenberg.org/files/2097/2097-0.txt\",\n",
    "        \"https://www.gutenberg.org/files/244/244-0.txt\"\n",
    "    ]\n",
    "    texts = []\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            text = response.text\n",
    "            start_idx = text.find(\"*** START\")\n",
    "            end_idx = text.find(\"*** END\")\n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                text = text[start_idx:end_idx]\n",
    "            texts.append(text)\n",
    "            print(f\"   \u2705 Downloaded book {i}: {len(text):,} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"   \u26a0\ufe0f Failed to download book {i}: {str(e)}\")\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "def create_numerical_data():\n",
    "    print(\"\\n\ud83d\udd22 Creating numerical training samples...\")\n",
    "    numerical_samples = []\n",
    "    for i in range(1000):\n",
    "        num1, num2 = i * 2, i * 3\n",
    "        numerical_samples.append(f\"Number {i}: {num1}, {num2}, sum={num1+num2}\")\n",
    "    for i in range(100, 200):\n",
    "        numerical_samples.append(f\"Calculation: {i} * 2 = {i*2}, {i} + 5 = {i+5}\")\n",
    "    text = \"\\n\".join(numerical_samples)\n",
    "    print(f\"   \u2705 Created {len(numerical_samples):,} numerical samples\")\n",
    "    return text\n",
    "\n",
    "sherlock_text = download_sherlock_holmes()\n",
    "numerical_text = create_numerical_data()\n",
    "\n",
    "print(\"\\n\ud83d\udcda Loading WikiText-2...\")\n",
    "try:\n",
    "    wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "    wikitext_text = \"\\n\".join([item['text'] for item in wikitext if item['text'].strip()][:10000])\n",
    "    print(f\"   \u2705 WikiText-2 loaded: {len(wikitext_text):,} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"   \u26a0\ufe0f WikiText-2 failed: {str(e)}. Using fallback text.\")\n",
    "    wikitext_text = \"This is sample text for training. \" * 1000\n",
    "\n",
    "print(\"\\n\ud83d\udcca Dataset Statistics:\")\n",
    "print(f\"   Sherlock Holmes: {len(sherlock_text):,} characters\")\n",
    "print(f\"   Numerical Data: {len(numerical_text):,} characters\")\n",
    "print(f\"   WikiText: {len(wikitext_text):,} characters\")\n",
    "\n",
    "combined_base_text = wikitext_text + \"\\n\\n\" + sherlock_text + \"\\n\\n\" + numerical_text\n",
    "print(f\"   Combined Base Training: {len(combined_base_text):,} characters\")\n",
    "\n",
    "fine_tune_text = sherlock_text + \"\\n\\n\" + numerical_text\n",
    "print(f\"   Fine-tuning Dataset: {len(fine_tune_text):,} characters\")\n",
    "\n",
    "print(\"\\n\u2705 All datasets loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Step 6: Dataset Tokenization and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_len=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(f\"\\n\ud83d\udd27 Tokenizing {len(text):,} characters...\")\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        print(f\"   \u2705 Created {len(self.tokens):,} tokens\")\n",
    "        self.num_samples = len(self.tokens) // max_len\n",
    "        print(f\"   \ud83d\udce6 Dataset size: {self.num_samples:,} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.max_len\n",
    "        end_idx = start_idx + self.max_len + 1\n",
    "        if end_idx > len(self.tokens):\n",
    "            end_idx = len(self.tokens)\n",
    "            start_idx = end_idx - self.max_len - 1\n",
    "        chunk = self.tokens[start_idx:end_idx]\n",
    "        if len(chunk) < self.max_len + 1:\n",
    "            chunk = chunk + [0] * (self.max_len + 1 - len(chunk))\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "print(\"\ud83d\udce6 Creating base training dataset...\")\n",
    "base_dataset = TextDataset(combined_base_text, tokenizer, max_len=1024)\n",
    "\n",
    "print(\"\\n\ud83d\udce6 Creating fine-tuning dataset...\")\n",
    "finetune_dataset = TextDataset(fine_tune_text, tokenizer, max_len=1024)\n",
    "\n",
    "print(\"\\n\u2705 Datasets ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Step 7: Enhanced Model Saving/Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_enhanced_model(model, save_path, config, training_history=None, stage='base', tokenizer_info=None):\n",
    "    print(f\"\\n\ud83d\udcbe Saving enhanced {stage} model to: {save_path}\")\n",
    "    \n",
    "    if tokenizer_info is None:\n",
    "        tokenizer_info = {\n",
    "            'tokenizer_type': 'tiktoken',\n",
    "            'encoding': 'cl100k_base',\n",
    "            'vocab_size': VOCAB_SIZE,\n",
    "            'description': 'GPT-3/GPT-4 tokenizer (tiktoken cl100k_base)'\n",
    "        }\n",
    "    \n",
    "    enhanced_checkpoint = {\n",
    "        'metadata': {\n",
    "            'version': '3.0',\n",
    "            'stage': stage,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'platform': ENV,\n",
    "            'device': str(device),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'training_id': str(uuid.uuid4())[:8],\n",
    "        },\n",
    "        'model_architecture': {\n",
    "            'class_name': 'AdvancedLLM',\n",
    "            'components': ['RMSNorm', 'RoPE', 'SwiGLU', 'MultiHeadAttention', 'TransformerBlock'],\n",
    "            'vocab_size': model.vocab_size,\n",
    "            'emb_size': model.emb_size,\n",
    "            'n_layers': model.n_layers,\n",
    "            'n_heads': model.n_heads,\n",
    "            'max_len': model.max_len,\n",
    "            'use_gradient_checkpoint': model.use_gradient_checkpoint,\n",
    "            'total_parameters': model.get_num_params(),\n",
    "        },\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'training_config': config,\n",
    "        'tokenizer_config': tokenizer_info,\n",
    "        'training_history': training_history or {},\n",
    "        'generation_config': {\n",
    "            'temperature': 0.8,\n",
    "            'top_k': 50,\n",
    "            'top_p': 0.92,\n",
    "            'repetition_penalty': 1.2,\n",
    "            'no_repeat_ngram_size': 3,\n",
    "        },\n",
    "        'retraining_info': {\n",
    "            'can_continue_training': True,\n",
    "            'supported_datasets': ['literary', 'custom', 'wikitext', 'sherlock_holmes', 'numerical'],\n",
    "            'fine_tuning_ready': True,\n",
    "            'recommended_lr': 1e-4,\n",
    "            'recommended_batch_size': 8,\n",
    "        },\n",
    "        'datasets_used': {\n",
    "            'base_training': ['WikiText-2', 'Sherlock Holmes (Project Gutenberg)', 'Numerical Samples'],\n",
    "            'fine_tuning': ['Sherlock Holmes', 'Numerical Samples'] if stage == 'fine_tuned' else [],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(enhanced_checkpoint, save_path)\n",
    "    file_size = os.path.getsize(save_path) / (1024**2)\n",
    "    print(f\"\\n\u2705 Model saved successfully!\")\n",
    "    print(f\"   \ud83d\udcc1 File: {save_path}\")\n",
    "    print(f\"   \ud83d\udcbe Size: {file_size:.1f}MB\")\n",
    "    print(f\"   \ud83d\udd22 Parameters: {model.get_num_params():,}\")\n",
    "    print(f\"   \ud83c\udfaf Stage: {stage}\")\n",
    "    print(f\"   \ud83d\udd24 Tokenizer: {tokenizer_info['tokenizer_type']} ({tokenizer_info['encoding']})\")\n",
    "    return save_path\n",
    "\n",
    "def load_enhanced_model(load_path, device='cuda'):\n",
    "    print(f\"\\n\ud83d\udcc2 Loading enhanced model from: {load_path}\")\n",
    "    checkpoint = torch.load(load_path, map_location=device)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccb Model Information:\")\n",
    "    print(f\"   Version: {checkpoint['metadata']['version']}\")\n",
    "    print(f\"   Stage: {checkpoint['metadata']['stage']}\")\n",
    "    print(f\"   Created: {checkpoint['metadata']['created_at']}\")\n",
    "    print(f\"   Tokenizer: {checkpoint['tokenizer_config']['tokenizer_type']}\")\n",
    "    \n",
    "    arch = checkpoint['model_architecture']\n",
    "    \n",
    "    model = AdvancedLLM(\n",
    "        vocab_size=arch['vocab_size'],\n",
    "        emb_size=arch['emb_size'],\n",
    "        n_layers=arch['n_layers'],\n",
    "        n_heads=arch['n_heads'],\n",
    "        max_len=arch['max_len'],\n",
    "        use_gradient_checkpoint=arch.get('use_gradient_checkpoint', False)\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"\\n\u2705 Model loaded: {arch['total_parameters']:,} parameters\")\n",
    "    \n",
    "    return model, checkpoint['training_config'], checkpoint.get('training_history', {})\n",
    "\n",
    "print(\"\u2705 Enhanced save/load functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Step 8: Enhanced Training Function with Validation & Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return math.exp(min(loss, 20))\n",
    "\n",
    "def create_train_val_split(dataset, val_ratio=0.1):\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, (model.module.vocab_size if hasattr(model, \"module\") else model.vocab_size)), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_batches\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, training_history, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict() if hasattr(model, \"module\") else model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'training_history': training_history,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"   \ud83d\udcbe Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device='cuda'):\n",
    "    print(f\"\\n\ud83d\udcc2 Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint.get('scheduler_state_dict'):\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    training_history = checkpoint.get('training_history', {})\n",
    "    \n",
    "    print(f\"\u2705 Checkpoint loaded! Resuming from epoch {start_epoch}\")\n",
    "    return start_epoch, training_history\n",
    "\n",
    "def train_model(model, dataset, config, stage_name='base', resume_from=None, val_dataset=None):\n",
    "    print(f\"\\n\ud83c\udfaf Starting {stage_name} training...\")\n",
    "    print(f\"   Dataset size: {len(dataset):,} samples\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch size: {config['batch_size']}\")\n",
    "    print(f\"   Learning rate: {config['learning_rate']}\")\n",
    "    print(f\"   FP16: {config.get('use_fp16', False)}\")\n",
    "    print(f\"   Validation: {val_dataset is not None}\")\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    if val_dataset is None:\n",
    "        print(\"\\n\ud83d\udcca Creating train/validation split (90/10)...\")\n",
    "        dataset, val_dataset = create_train_val_split(dataset, val_ratio=0.1)\n",
    "        print(f\"   Train: {len(dataset):,} samples\")\n",
    "        print(f\"   Validation: {len(val_dataset):,} samples\")\n",
    "    \n",
    "    # Setup distributed sampler for DDP\n",
    "    train_sampler = None\n",
    "    val_sampler = None\n",
    "    if USE_MULTI_GPU:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"   Model wrapped with DataParallel for {torch.cuda.device_count()} GPUs\")\n",
    "    # val_sampler = DistributedSampler(val_dataset, shuffle=False)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=config['batch_size'], sampler=train_sampler, shuffle=(train_sampler is None), \n",
    "                             num_workers=0, pin_memory=True if device == 'cuda' else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], sampler=val_sampler, shuffle=False,\n",
    "                           num_workers=0, pin_memory=True if device == 'cuda' else False)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], \n",
    "                                 weight_decay=config.get('weight_decay', 0.01))\n",
    "    \n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        return 0.5 * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    scaler = GradScaler() if config.get('use_fp16', False) and device == 'cuda' else None\n",
    "    \n",
    "    start_epoch = 0\n",
    "    training_history = {\n",
    "        'stage': stage_name,\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'val_perplexities': [],\n",
    "        'epochs': [],\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    if resume_from and os.path.exists(resume_from):\n",
    "        start_epoch, loaded_history = load_checkpoint(model, optimizer, scheduler, resume_from, device)\n",
    "        training_history.update(loaded_history)\n",
    "    \n",
    "    global_step = 0\n",
    "    best_val_loss = training_history.get('best_val_loss', float('inf'))\n",
    "    avg_train_loss = training_history.get('final_train_loss', None)\n",
    "    val_loss = training_history.get('final_val_loss', None)\n",
    "    \n",
    "    for epoch in range(start_epoch, config['epochs']):\n",
    "        # Set epoch for distributed sampler to ensure proper shuffling\n",
    "        # DataParallel doesn't need epoch setting\n",
    "        pass  # Using DataParallel, not DistributedSampler\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(progress_bar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if scaler:\n",
    "                with autocast():\n",
    "                    logits = model(x)\n",
    "                    loss = F.cross_entropy(logits.view(-1, (model.module.vocab_size if hasattr(model, \"module\") else model.vocab_size)), y.view(-1))\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.view(-1, (model.module.vocab_size if hasattr(model, \"module\") else model.vocab_size)), y.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
    "            })\n",
    "            \n",
    "            if batch_idx % 100 == 0 and device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_perplexity = calculate_perplexity(avg_train_loss)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Evaluating on validation set...\")\n",
    "        val_loss, val_perplexity = evaluate_model(model, val_loader, device)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        training_history['train_losses'].append(avg_train_loss)\n",
    "        training_history['val_losses'].append(val_loss)\n",
    "        training_history['val_perplexities'].append(val_perplexity)\n",
    "        training_history['epochs'].append(epoch + 1)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Epoch {epoch+1}/{config['epochs']} Results:\")\n",
    "        print(f\"   Train Loss: {avg_train_loss:.4f} | Perplexity: {train_perplexity:.2f}\")\n",
    "        print(f\"   Val Loss: {val_loss:.4f} | Perplexity: {val_perplexity:.2f}\")\n",
    "        print(f\"   Time: {epoch_time:.1f}s | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{stage_name}_checkpoint_epoch_{epoch+1}.pt\")\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, training_history, checkpoint_path)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"{stage_name}_best.pt\")\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, training_history, best_checkpoint_path)\n",
    "            print(f\"   \ud83c\udf89 New best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    training_history['end_time'] = datetime.now().isoformat()\n",
    "    \n",
    "    if avg_train_loss is not None and val_loss is not None:\n",
    "        training_history['best_val_loss'] = best_val_loss\n",
    "        training_history['final_train_loss'] = avg_train_loss\n",
    "        training_history['final_val_loss'] = val_loss\n",
    "    print(f\"\\n\u2705 {stage_name.capitalize()} training completed!\")\n",
    "    if start_epoch < config['epochs']:\n",
    "        if avg_train_loss is not None and val_loss is not None:\n",
    "            print(f\"   Best Val Loss: {best_val_loss:.4f}\")\n",
    "            print(f\"   Final Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"   Final Val Loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"   Resumed from final epoch - training already complete\")\n",
    "        if 'best_val_loss' in training_history:\n",
    "            print(f\"   Best Val Loss: {training_history['best_val_loss']:.4f}\")\n",
    "        if 'final_val_loss' in training_history:\n",
    "            print(f\"   Final Val Loss: {training_history['final_val_loss']:.4f}\")\n",
    "    return training_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Step 9: Initialize Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude80 Initializing model...\")\n",
    "\n",
    "model = AdvancedLLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_size=768,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    ff_size=3072,\n",
    "    max_len=1024,\n",
    "    dropout=0.1,\n",
    "    use_gradient_checkpoint=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n\u2705 Model initialized!\")\n",
    "print(f\"   Total parameters: {model.get_num_params():,}\")\n",
    "print(f\"   Model size: ~{model.get_num_params() * 4 / (1024**2):.1f}MB (FP32)\")\n",
    "\n",
    "base_config = {\n",
    "    'epochs': 3,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'use_fp16': device == 'cuda',\n",
    "    'max_len': 1024,\n",
    "    'dataset': 'WikiText-2 + Sherlock Holmes + Numerical',\n",
    "}\n",
    "\n",
    "finetune_config = {\n",
    "    'epochs': 2,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'use_fp16': device == 'cuda',\n",
    "    'max_len': 1024,\n",
    "    'dataset': 'Sherlock Holmes + Numerical (Fine-tuning)',\n",
    "}\n",
    "\n",
    "print(\"\\n\ud83d\udccb Training Configuration:\")\n",
    "print(f\"   Base Training: {base_config['epochs']} epochs, LR={base_config['learning_rate']}\")\n",
    "print(f\"   Fine-tuning: {finetune_config['epochs']} epochs, LR={finetune_config['learning_rate']}\")\n",
    "print(f\"   Batch size: {base_config['batch_size']}\")\n",
    "print(f\"   FP16: {base_config['use_fp16']}\")\n",
    "\n",
    "monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfcb\ufe0f Step 10: Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udfcb\ufe0f STAGE 1: BASE MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "base_history = train_model(model, base_dataset, base_config, stage_name='base')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcbe Saving base model as final.pt\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "base_model_path = os.path.join(MODEL_DIR, \"final.pt\")\n",
    "save_enhanced_model(\n",
    "    model=model,\n",
    "    save_path=base_model_path,\n",
    "    config=base_config,\n",
    "    training_history=base_history,\n",
    "    stage='base_trained',\n",
    "    tokenizer_info={\n",
    "        'tokenizer_type': 'tiktoken',\n",
    "        'encoding': 'cl100k_base',\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'description': 'GPT-3/GPT-4 tokenizer (tiktoken cl100k_base)'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Base model training complete!\")\n",
    "print(f\"   Model saved to: {base_model_path}\")\n",
    "monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfa8 Step 11: Automatic Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udfa8 STAGE 2: FINE-TUNING ON LITERARY DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\ud83d\udcc2 Loading base model for fine-tuning...\")\n",
    "model_ft, loaded_config, loaded_history = load_enhanced_model(base_model_path, device=device)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Starting fine-tuning...\")\n",
    "finetune_history = train_model(model_ft, finetune_dataset, finetune_config, stage_name='fine-tuning')\n",
    "\n",
    "combined_history = {\n",
    "    'base_training': base_history,\n",
    "    'fine_tuning': finetune_history,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcbe Saving fine-tuned model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "finetuned_model_path = os.path.join(MODEL_DIR, \"final_finetuned.pt\")\n",
    "save_enhanced_model(\n",
    "    model=model_ft,\n",
    "    save_path=finetuned_model_path,\n",
    "    config=finetune_config,\n",
    "    training_history=combined_history,\n",
    "    stage='fine_tuned',\n",
    "    tokenizer_info={\n",
    "        'tokenizer_type': 'tiktoken',\n",
    "        'encoding': 'cl100k_base',\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'description': 'GPT-3/GPT-4 tokenizer (tiktoken cl100k_base)'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf89 TRAINING PIPELINE COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n\ud83d\udce6 Models saved:\")\n",
    "print(f\"   Base model: {base_model_path}\")\n",
    "print(f\"   Fine-tuned model: {finetuned_model_path}\")\n",
    "print(f\"\\n\ud83d\udcca Training Summary:\")\n",
    "print(f\"   Base training loss: {base_history['final_loss']:.4f}\")\n",
    "print(f\"   Fine-tuning loss: {finetune_history['final_loss']:.4f}\")\n",
    "print(f\"   Total parameters: {model_ft.get_num_params():,}\")\n",
    "print(f\"   Tokenizer: tiktoken (cl100k_base - GPT-3/GPT-4)\")\n",
    "\n",
    "monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Step 12: Test Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83e\uddea Testing text generation...\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model, prompt, max_tokens=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        if input_ids.size(1) >= model.max_len:\n",
    "            input_ids = input_ids[:, -model.max_len:]\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    generated_tokens = input_ids[0].tolist()\n",
    "    return tokenizer.decode(generated_tokens)\n",
    "\n",
    "test_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"Sherlock Holmes\",\n",
    "    \"The number\",\n",
    "]\n",
    "\n",
    "print(\"\\n\ud83d\udcdd Generated samples:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    generated = generate_text(model_ft, prompt, max_tokens=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\u2705 Text generation test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Step 13: Model Information Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udccb FINAL MODEL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checkpoint = torch.load(finetuned_model_path, map_location='cpu')\n",
    "\n",
    "print(\"\\n\ud83d\udd27 Architecture:\")\n",
    "for key, value in checkpoint['model_architecture'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd24 Tokenizer:\")\n",
    "for key, value in checkpoint['tokenizer_config'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Training History:\")\n",
    "if 'base_training' in checkpoint['training_history']:\n",
    "    print(f\"   Base training epochs: {len(checkpoint['training_history']['base_training'].get('train_losses', []))}\")\n",
    "    print(f\"   Fine-tuning epochs: {len(checkpoint['training_history']['fine_tuning'].get('train_losses', []))}\")\n",
    "    print(f\"   Final base val loss: {checkpoint['training_history']['base_training'].get('final_val_loss', 'N/A')}\")\n",
    "    print(f\"   Final fine-tuning val loss: {checkpoint['training_history']['fine_tuning'].get('final_val_loss', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcda Datasets Used:\")\n",
    "for key, datasets in checkpoint['datasets_used'].items():\n",
    "    print(f\"   {key}:\")\n",
    "    for dataset in datasets:\n",
    "        print(f\"      - {dataset}\")\n",
    "\n",
    "print(\"\\n\ud83d\udd04 Retraining Information:\")\n",
    "for key, value in checkpoint['retraining_info'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\ud83d\udce6 Files:\")\n",
    "for filename in os.listdir(MODEL_DIR):\n",
    "    filepath = os.path.join(MODEL_DIR, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        size = os.path.getsize(filepath) / (1024**2)\n",
    "        print(f\"   {filename}: {size:.1f}MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 ALL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYour model is ready to use! The final.pt file contains:\")\n",
    "print(\"  \u2705 Complete model architecture and weights\")\n",
    "print(\"  \u2705 GPT-3 tokenizer configuration (tiktoken cl100k_base)\")\n",
    "print(\"  \u2705 Full training history and metadata\")\n",
    "print(\"  \u2705 Everything needed for future retraining\")\n",
    "print(\"  \u2705 Validation metrics and perplexity scores\")\n",
    "print(\"  \u2705 Checkpoint recovery for interrupted training\")\n",
    "print(\"\\nSee cells below for:\")\n",
    "print(\"  - Continue training with new datasets\")\n",
    "print(\"  - Enhanced text generation interface\")\n",
    "print(\"  - Custom dataset upload support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# \ud83d\udd04 ADVANCED FEATURES\n",
    "## Use the cells below for additional functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd01 Continue Training from Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd01 CONTINUE TRAINING WORKFLOW\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis cell demonstrates how to load a saved model and continue training.\")\n",
    "print(\"You can add any new dataset and continue training from where you left off.\\n\")\n",
    "\n",
    "model_to_continue = input(\"Enter model path to load (or press Enter for latest): \").strip()\n",
    "if not model_to_continue:\n",
    "    model_to_continue = finetuned_model_path\n",
    "\n",
    "print(f\"\\n\ud83d\udcc2 Loading model from: {model_to_continue}\")\n",
    "continued_model, cont_config, cont_history = load_enhanced_model(model_to_continue, device=device)\n",
    "\n",
    "print(\"\\n\ud83d\udcdd Add your custom training data below:\")\n",
    "print(\"Example: additional_text = 'Your new training data here...'\")\n",
    "additional_text = input(\"Enter additional training text (or press Enter to skip): \").strip()\n",
    "\n",
    "if additional_text:\n",
    "    print(f\"\\n\ud83d\udd27 Creating dataset from new text ({len(additional_text)} characters)...\")\n",
    "    continue_dataset = TextDataset(additional_text, tokenizer, max_len=1024)\n",
    "    \n",
    "    continue_config = {\n",
    "        'epochs': 2,\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 5e-5,\n",
    "        'weight_decay': 0.01,\n",
    "        'use_fp16': device == 'cuda',\n",
    "        'max_len': 512,\n",
    "        'dataset': 'Additional Custom Data',\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf Continuing training with new data...\")\n",
    "    continue_history = train_model(continued_model, continue_dataset, continue_config, stage_name='continued')\n",
    "    \n",
    "    continued_model_path = os.path.join(MODEL_DIR, \"continued_model.pt\")\n",
    "    save_enhanced_model(\n",
    "        model=continued_model,\n",
    "        save_path=continued_model_path,\n",
    "        config=continue_config,\n",
    "        training_history={'previous': cont_history, 'continued': continue_history},\n",
    "        stage='continued_training',\n",
    "        tokenizer_info={\n",
    "            'tokenizer_type': 'tiktoken',\n",
    "            'encoding': 'cl100k_base',\n",
    "            'vocab_size': VOCAB_SIZE,\n",
    "            'description': 'GPT-3/GPT-4 tokenizer (tiktoken cl100k_base)'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\u2705 Continued training complete! Model saved to: {continued_model_path}\")\n",
    "else:\n",
    "    print(\"\\n\u23ed\ufe0f Skipping continued training. Model loaded and ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfa8 Enhanced Text Generation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTextGenerator:\n",
    "    def __init__(self, model, tokenizer, device='cuda'):\n",
    "        self.model = model.to(device).eval()\n",
    "        if hasattr(model, 'module'):  # Unwrap DDP\n",
    "            self.model = model.module\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt, max_tokens=150, temperature=0.8, top_k=50, \n",
    "                 top_p=0.92, repetition_penalty=1.2, num_samples=1):\n",
    "        results = []\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            tokens = self.tokenizer.encode(prompt)\n",
    "            input_ids = torch.tensor([tokens]).to(self.device)\n",
    "            \n",
    "            past_tokens = set()\n",
    "            \n",
    "            for _ in range(max_tokens):\n",
    "                if input_ids.size(1) >= self.model.max_len:\n",
    "                    input_ids = input_ids[:, -self.model.max_len:]\n",
    "                \n",
    "                logits = self.model(input_ids)\n",
    "                next_token_logits = logits[:, -1, :].float()\n",
    "                \n",
    "                if repetition_penalty != 1.0:\n",
    "                    for token_id in past_tokens:\n",
    "                        next_token_logits[:, token_id] /= repetition_penalty\n",
    "                \n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                \n",
    "                if top_k > 0:\n",
    "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][:, -1, None]\n",
    "                    next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                if top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                    sorted_indices_to_remove[:, 0] = 0\n",
    "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                    next_token_logits[:, indices_to_remove] = float('-inf')\n",
    "                \n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                past_tokens.add(next_token.item())\n",
    "            \n",
    "            generated_tokens = input_ids[0].tolist()\n",
    "            generated_text = self.tokenizer.decode(generated_tokens)\n",
    "            results.append(generated_text)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"\ud83c\udfa8 Enhanced Text Generation Interface\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "generator = EnhancedTextGenerator(model_ft, tokenizer, device=device)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Interactive Generation Mode\")\n",
    "print(\"\\nEnter prompts to generate text. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_prompt = input(\"\\nPrompt (or 'quit'): \").strip()\n",
    "    if user_prompt.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    if not user_prompt:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        max_tokens = int(input(\"Max tokens (default 100): \") or \"100\")\n",
    "        temperature = float(input(\"Temperature 0.1-2.0 (default 0.8): \") or \"0.8\")\n",
    "        num_samples = int(input(\"Number of samples (default 1): \") or \"1\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd04 Generating {num_samples} sample(s)...\\n\")\n",
    "        \n",
    "        results = generator.generate(\n",
    "            prompt=user_prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        \n",
    "        for i, text in enumerate(results, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(text)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"\u274c Invalid input: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n\u23f8\ufe0f Generation interrupted.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\u2705 Text generation interface closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce4 Custom Dataset Upload & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udce4 CUSTOM DATASET UPLOAD & PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis cell allows you to upload and preprocess custom datasets for training.\\n\")\n",
    "\n",
    "def preprocess_custom_dataset(text, min_length=10, max_length=1000):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if min_length <= len(line) <= max_length:\n",
    "            filtered_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "def load_custom_dataset_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_custom_dataset_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error downloading from URL: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Choose dataset source:\")\n",
    "print(\"1. Enter text directly\")\n",
    "print(\"2. Load from file path\")\n",
    "print(\"3. Download from URL\")\n",
    "\n",
    "choice = input(\"\\nEnter choice (1/2/3): \").strip()\n",
    "\n",
    "custom_text = None\n",
    "\n",
    "if choice == '1':\n",
    "    print(\"\\nEnter your text (type 'END' on a new line to finish):\\n\")\n",
    "    lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line.strip() == 'END':\n",
    "            break\n",
    "        lines.append(line)\n",
    "    custom_text = '\\n'.join(lines)\n",
    "\n",
    "elif choice == '2':\n",
    "    file_path = input(\"\\nEnter file path: \").strip()\n",
    "    custom_text = load_custom_dataset_from_file(file_path)\n",
    "\n",
    "elif choice == '3':\n",
    "    url = input(\"\\nEnter URL: \").strip()\n",
    "    custom_text = load_custom_dataset_from_url(url)\n",
    "\n",
    "else:\n",
    "    print(\"\u274c Invalid choice\")\n",
    "\n",
    "if custom_text:\n",
    "    print(f\"\\n\ud83d\udcca Dataset loaded: {len(custom_text):,} characters\")\n",
    "    \n",
    "    preprocess = input(\"\\nPreprocess dataset? (y/n): \").strip().lower() == 'y'\n",
    "    \n",
    "    if preprocess:\n",
    "        min_len = int(input(\"Minimum line length (default 10): \") or \"10\")\n",
    "        max_len = int(input(\"Maximum line length (default 1000): \") or \"1000\")\n",
    "        custom_text = preprocess_custom_dataset(custom_text, min_len, max_len)\n",
    "        print(f\"\u2705 Preprocessed: {len(custom_text):,} characters\")\n",
    "    \n",
    "    save_path = os.path.join(DATA_DIR, \"custom_dataset.txt\")\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(custom_text)\n",
    "    print(f\"\\n\ud83d\udcbe Custom dataset saved to: {save_path}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udd27 Creating training dataset...\")\n",
    "    custom_dataset = TextDataset(custom_text, tokenizer, max_len=1024)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Dataset ready for training!\")\n",
    "    print(f\"   Characters: {len(custom_text):,}\")\n",
    "    print(f\"   Samples: {len(custom_dataset):,}\")\n",
    "    print(\"\\nYou can now use 'custom_dataset' in the training function.\")\n",
    "    print(\"Example: train_model(model, custom_dataset, config, stage_name='custom')\")\n",
    "else:\n",
    "    print(\"\\n\u23ed\ufe0f No dataset loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude91 Checkpoint Recovery Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude91 CHECKPOINT RECOVERY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis cell shows how to recover from an interrupted training session.\\n\")\n",
    "\n",
    "print(\"\ud83d\udcc1 Available checkpoints:\\n\")\n",
    "checkpoint_files = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pt')]\n",
    "\n",
    "if checkpoint_files:\n",
    "    for i, ckpt in enumerate(checkpoint_files, 1):\n",
    "        ckpt_path = os.path.join(CHECKPOINT_DIR, ckpt)\n",
    "        size = os.path.getsize(ckpt_path) / (1024**2)\n",
    "        print(f\"{i}. {ckpt} ({size:.1f}MB)\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 To resume training from a checkpoint:\")\n",
    "    print(\"\\n# Load the model\")\n",
    "    print(\"recovery_model, _, _ = load_enhanced_model(base_model_path, device=device)\")\n",
    "    print(\"\\n# Resume training with the checkpoint\")\n",
    "    print(\"checkpoint_path = os.path.join(CHECKPOINT_DIR, 'base_checkpoint_epoch_1.pt')\")\n",
    "    print(\"recovery_history = train_model(\")\n",
    "    print(\"    model=recovery_model,\")\n",
    "    print(\"    dataset=base_dataset,\")\n",
    "    print(\"    config=base_config,\")\n",
    "    print(\"    stage_name='recovered',\")\n",
    "    print(\"    resume_from=checkpoint_path\")\n",
    "    print(\")\")\n",
    "    \n",
    "    print(\"\\n\u2705 The training will automatically resume from the saved epoch!\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Checkpoints are saved during training.\")\n",
    "\n",
    "print(\"\\n\ud83d\udccb Checkpoint files contain:\")\n",
    "print(\"   - Model state (weights)\")\n",
    "print(\"   - Optimizer state\")\n",
    "print(\"   - Scheduler state\")\n",
    "print(\"   - Training history\")\n",
    "print(\"   - Current epoch number\")\n",
    "print(\"\\nThis allows seamless recovery from any interruption!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddf9 Step 19: Cleanup DataParallel Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU resources\n",
    "if device.startswith('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    if USE_MULTI_GPU:\n",
    "        print(f\"\u2705 Cleared cache on {torch.cuda.device_count()} GPUs\")\n",
    "    else:\n",
    "        print(\"\u2705 GPU cache cleared\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Training pipeline completed successfully!\")\n",
    "print(f\"   Final model saved: {os.path.join(MODEL_DIR, 'final_finetuned.pt')}\")\n",
    "print(f\"   Model parameters: ~{(768 * 8 * 8 * 3072 * 4) / (1024**2):.0f}M\")\n"
   ]
  }
 ]
}